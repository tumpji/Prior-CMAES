{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d212fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import itertools, functools\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "sys.path.append('convert_full_data')\n",
    "\n",
    "import tss\n",
    "\n",
    "from parallization_metacentrum import ArrayIsMetacentrum, IsMetacentrum\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3212621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordedRun:\n",
    "    def __init__(self, npz_path, info):\n",
    "        super().__init__()\n",
    "\n",
    "        npz = np.load(npz_path)\n",
    "\n",
    "        self.dim = npz['dimensions']\n",
    "        self.fun = npz['function_id']\n",
    "\n",
    "        self.xmeans = npz['surrogate_data_means'].T\n",
    "        self.sigmas = npz['surrogate_data_sigmas']\n",
    "        self.bds = npz['surrogate_data_bds']\n",
    "        self.iruns = npz['iruns']\n",
    "        self.evals = npz['evals']\n",
    "        self.points = npz['points']\n",
    "        self.fvalues = npz['fvalues']\n",
    "        self.orig = npz['orig_evaled']\n",
    "        self.coco = npz['fvalues_orig']\n",
    "        self.gen_split = npz['gen_split']\n",
    "\n",
    "        self.n_gen = len(self.gen_split)\n",
    "        self.n_points = len(self.points)\n",
    "\n",
    "        # tuple: x_train, y_train, x_test, y_test, tss_mask, stats\n",
    "        self.all_gens = [self.get_gen(gen_i, info) for gen_i in range(1, self.n_gen)]\n",
    "        self.stats_table = pd.DataFrame([gen[-1] for gen in self.all_gens])\n",
    "\n",
    "        # for key, val in info.items():\n",
    "        #     self.stats_table[key] = val\n",
    "        # self.stats_table['ins'] = ins\n",
    "\n",
    "    def get_gen(self, gen_i, info):\n",
    "        # first point is initial guess\n",
    "        # gen_split[0] = 0, gen_split[1] = 1, gen_split[2] = 1, ...\n",
    "        low = self.gen_split[gen_i] + 1\n",
    "        high = self.gen_split[gen_i + 1] + 1 if gen_i + 1 < self.n_gen else self.n_points\n",
    "\n",
    "        x_test = self.points[low:high]\n",
    "        y_test = self.coco[low:high]\n",
    "\n",
    "        o = self.orig[:low]\n",
    "        x_train = self.points[:low][o]\n",
    "        y_train = self.coco[:low][o]\n",
    "\n",
    "        pop = x_test\n",
    "\n",
    "        mean = self.xmeans[gen_i]\n",
    "        sigma = self.sigmas[gen_i]\n",
    "        bd = self.bds[gen_i]\n",
    "        mahalanobis_transf = np.linalg.inv(bd * sigma)\n",
    "\n",
    "        maximum_distance = 4  # trainRange\n",
    "        maximum_number = int(20 * self.dim)\n",
    "\n",
    "        tss2 = tss.TSS2(pop, mahalanobis_transf, maximum_distance, maximum_number)\n",
    "        tss_mask, _ = tss2(x_train, y_train)\n",
    "\n",
    "        x_tss = x_train[tss_mask]\n",
    "        y_tss = y_train[tss_mask]\n",
    "\n",
    "        stats = {\n",
    "            \"gen_num\": gen_i,\n",
    "            \"restarts\": self.iruns[gen_i] - 1,\n",
    "            \"arch_len\": len(x_train),\n",
    "            \"tss_len\": np.count_nonzero(tss_mask),\n",
    "            \"var_y_tss\": np.var(y_tss),\n",
    "\n",
    "            \"sigma\": sigma,\n",
    "            \"bd\": bd,\n",
    "            \"mean\": mean,\n",
    "            #\"cond_num\": condition_num(bd.T),\n",
    "            #\"fst_scnd_ratio\": fst_scnd_ratio(bd.T),\n",
    "            #\"max_eigenvec\": max(np.linalg.norm(bd.T, axis=1))\n",
    "        }\n",
    "        stats.update(info)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test, tss_mask, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff314d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5280\n"
     ]
    }
   ],
   "source": [
    "run_folder = 'data_full/'\n",
    "run_files = os.listdir(run_folder)\n",
    "run_files = natsorted(run_files)\n",
    "print(len(run_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b84e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossRDE:\n",
    "    name = 'RDE'\n",
    "    cache = {}\n",
    "    \n",
    "    def __init__(self, mu):\n",
    "        self.mu = mu\n",
    "\n",
    "    @property\n",
    "    def mu(self):\n",
    "        return self._mu\n",
    "\n",
    "    @mu.setter\n",
    "    def mu(self, mu):\n",
    "        assert isinstance(mu, int)\n",
    "        self._mu = mu\n",
    "        \n",
    "    def _compute_normalization_coefficient(self, lam, mu):\n",
    "        assert mu <= lam\n",
    "        \n",
    "        prvni_sloupec = np.arange(1, -mu, step=-1)[:, np.newaxis]\n",
    "        assert len(prvni_sloupec) == mu + 1\n",
    "        \n",
    "        radek = np.arange(1,mu+1)[np.newaxis, :]\n",
    "        radek_obraceny = np.arange(lam, lam-mu, step=-1)[np.newaxis, :]\n",
    "        assert radek.shape[1] == mu\n",
    "        assert radek_obraceny.shape[1] == mu\n",
    "        \n",
    "        tabulka = prvni_sloupec + (radek - 1)\n",
    "        tabulka = np.where(tabulka > 0, tabulka, radek_obraceny)\n",
    "        vysledek = np.amax(np.sum(np.abs(tabulka - radek), axis=1))\n",
    "        return vysledek\n",
    "    \n",
    "    def __call__(self, predict, target):\n",
    "        super().__call__(predict, target)\n",
    "        lam = len(predict)\n",
    "        try:\n",
    "            err_max = self.cache[(lam, self._mu)]\n",
    "        except KeyError:\n",
    "            err_max = self._compute_normalization_coefficient(lam, self.mu)\n",
    "            self.cache[(lam, self._mu)] = err_max\n",
    "            \n",
    "        si_predict = np.argsort(predict)\n",
    "        si_target  = np.argsort(target)[:self._mu]\n",
    "        \n",
    "        inRank = np.zeros(lam)\n",
    "        inRank[si_predict] = np.arange(lam)\n",
    "        \n",
    "        r1 = inRank[si_target[:self._mu]]\n",
    "        r2 = np.arange(self._mu)\n",
    "        return np.sum(np.abs(r1 - r2))/err_max\n",
    "\n",
    "class LossRDE_auto(LossRDE):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, predict, target):\n",
    "        lam = len(target)\n",
    "        self.mu = int(math.floor(lam / 2))\n",
    "        return super().__call__(predict, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cfddc",
   "metadata": {},
   "source": [
    "### stuff used for ens and distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930514ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee131763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probabilistic_models import extend_info, \\\n",
    "    get_information_basic_model, get_information_ens, \\\n",
    "    get_information_distillation, get_information_double_distillation, \\\n",
    "    train_NLL_ensemble, train_ensemble, train_distillation, train_double_distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51bb6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models_on_specific_state(state):\n",
    "    training_output = {}\n",
    "    \n",
    "    # dataset handling\n",
    "    x_train, y_train, x_test, y_test, tss_mask, stats = state\n",
    "        \n",
    "    x = x_train[tss_mask].astype(np.float32)\n",
    "    y = y_train[tss_mask].astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    training_output['train_size'] = len(x)\n",
    "    training_output['test_size'] = len(x_test)\n",
    "\n",
    "    x_train, x_validation, y_train, y_validation = \\\n",
    "        sklearn.model_selection.train_test_split(x,y)\n",
    "    \n",
    "    x_train = torch.tensor(x_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    x_validation = torch.tensor(x_validation)\n",
    "    y_validation = torch.tensor(y_validation)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_validation.shape)\n",
    "    print(y_validation.shape)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    validation_dataset = torch.utils.data.TensorDataset(x_validation, y_validation)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=5, shuffle=True\n",
    "        )\n",
    "    validation_dataloader = torch.utils.data.DataLoader(\n",
    "            validation_dataset, batch_size=5, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # train ensemble\n",
    "    ensemble = train_NLL_ensemble(train_dataloader, validation_dataloader)\n",
    "\n",
    "    # ---------------------\n",
    "    \n",
    "    # 1. use only one model\n",
    "    model = ensemble[0]\n",
    "    training_output.update(\n",
    "        extend_info('basic', get_information_basic_model, x_test, y_test)\n",
    "    )\n",
    "    \n",
    "    # 2. use ensemble\n",
    "    model = train_ensemble(ensemble)\n",
    "    training_output.update(\n",
    "        extend_info('ensemble', get_information_ens, x_test, y_test)\n",
    "    )\n",
    "\n",
    "    # 3. use distillation\n",
    "    model = train_distillation(ensemble, train_dataloader, validation_dataloader)\n",
    "    training_output.update(\n",
    "        extend_info('distillation', get_information_distillation, x_test, y_test)\n",
    "    )\n",
    "\n",
    "    # 4. use double distillation\n",
    "    model = train_double_distillation(ensemble, train_dataloader, validation_dataloader)\n",
    "    training_output.update(\n",
    "        extend_info('doubledistillation', get_information_double_distillation, x_test, y_test)\n",
    "    )\n",
    "    \n",
    "    return training_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d1ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(name):\n",
    "    npz = np.load(run_folder + name)\n",
    "\n",
    "    fun, dim, rid, ins = name.split('.')[0].split('_')[-4:]\n",
    "    \n",
    "    fun = int(fun)\n",
    "    dim = int(dim[:-1])\n",
    "    ker = (int(rid) - 1) % 9\n",
    "    ins = int(ins)\n",
    "    \n",
    "    run_dict = {\n",
    "                'fun': fun,\n",
    "                'dim': dim,\n",
    "                'ker': ker,\n",
    "                'ins': ins\n",
    "            }\n",
    "    \n",
    "    record = RecordedRun(run_folder +  name, run_dict)\n",
    "    \n",
    "    output_folder = 'test_output/'\n",
    "    \n",
    "    with open(output_folder + name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        for ig, generation in enumerate(record.all_gens):\n",
    "            output = train_all_models_on_specific_state(generation)\n",
    "            return output\n",
    "            fields = [name, ig, *output]\n",
    "            # save results\n",
    "            writer.writerow(fields)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03d2df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n",
      "[tensor([[ 2.4467,  3.3691],\n",
      "        [ 2.4219, -3.4063],\n",
      "        [ 4.1540, -2.6965],\n",
      "        [-1.8079, -3.2079],\n",
      "        [ 2.3282, -2.0042]]), tensor([272.9194, 221.0556, 223.1950, 244.0245, 224.3768])]\n",
      "Starting training for 600 steps\n",
      "[tensor([[-1.8079, -3.2079],\n",
      "        [ 2.3282, -2.0042],\n",
      "        [ 2.4219, -3.4063],\n",
      "        [ 4.1540, -2.6965],\n",
      "        [ 2.4467,  3.3691]]), tensor([244.0245, 224.3768, 221.0556, 223.1950, 272.9194])]\n",
      "Starting training for 600 steps\n",
      "[tensor([[-1.8079, -3.2079],\n",
      "        [ 2.3282, -2.0042],\n",
      "        [ 2.4467,  3.3691],\n",
      "        [ 4.1540, -2.6965],\n",
      "        [ 2.4219, -3.4063]]), tensor([244.0245, 224.3768, 272.9194, 223.1950, 221.0556])]\n",
      "Starting training for 600 steps\n",
      "[tensor([[ 4.1540, -2.6965],\n",
      "        [ 2.3282, -2.0042],\n",
      "        [ 2.4467,  3.3691],\n",
      "        [-1.8079, -3.2079],\n",
      "        [ 2.4219, -3.4063]]), tensor([223.1950, 224.3768, 272.9194, 244.0245, 221.0556])]\n",
      "Starting training for 600 steps\n",
      "[tensor([[ 2.3282, -2.0042],\n",
      "        [ 2.4219, -3.4063],\n",
      "        [ 4.1540, -2.6965],\n",
      "        [ 2.4467,  3.3691],\n",
      "        [-1.8079, -3.2079]]), tensor([224.3768, 221.0556, 223.1950, 272.9194, 244.0245])]\n",
      "Starting training for 600 steps\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "extend_info() missing 1 required positional argument: 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     23\u001b[0m writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(f)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ig, generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(record\u001b[38;5;241m.\u001b[39mall_gens):\n\u001b[0;32m---> 26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_all_models_on_specific_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m     28\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [name, ig, \u001b[38;5;241m*\u001b[39moutput]\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain_all_models_on_specific_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 1. use only one model\u001b[39;00m\n\u001b[1;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m ensemble[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m training_output\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mextend_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_information_basic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 2. use ensemble\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m train_ensemble(ensemble)\n",
      "File \u001b[0;32m~/bin/python_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: extend_info() missing 1 required positional argument: 'y_test'"
     ]
    }
   ],
   "source": [
    "process_dataset(run_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d25a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensor in train_dataloader:\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d3399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ba1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_names = ['lin', 'quad', 'se', 'matern5', 'rq', 'nn-arcsin', 'add', 'se+quad', 'gibbs']\n",
    "\n",
    "for name in list(itertools.islice(run_files, 2, 10)):\n",
    "    npz = np.load(run_folder + name)\n",
    "\n",
    "    fun, dim, rid, ins = name.split('.')[0].split('_')[-4:]\n",
    "    \n",
    "    fun = int(fun)\n",
    "    dim = int(dim[:-1])\n",
    "    ker = (int(rid) - 1) % 9\n",
    "    ins = int(ins)\n",
    "    \n",
    "    run_dict = {\n",
    "                'fun': fun,\n",
    "                'dim': dim,\n",
    "                'ker': ker,\n",
    "                'ins': ins\n",
    "            }\n",
    "    \n",
    "    record = RecordedRun(run_folder +  name, run_dict)\n",
    "    \n",
    "    print(run_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de575003",
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordedRun(run_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapaths = list(map(lambda x: 'data/' + x, os.listdir('data')))\n",
    "print(datapaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb57a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in itertools.islice(datapaths, 1):\n",
    "    \n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    \n",
    "    for state in data:\n",
    "        x_train, y_train, x_test, y_test, tss_mask, stats = state\n",
    "        \n",
    "        x = x_train[tss_mask]\n",
    "        y = y_train[tss_mask]\n",
    "        \n",
    "        x_test\n",
    "        y_test\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
