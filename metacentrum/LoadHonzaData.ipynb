{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d212fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# io\n",
    "import os, sys, time\n",
    "import itertools, functools, collections\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "import csv\n",
    "\n",
    "sys.path.append('convert_full_data')\n",
    "sys.path.append('../regression-prior-networks')\n",
    "\n",
    "# science utils\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#####################\n",
    "# custom\n",
    "import tss\n",
    "from parallization_metacentrum import ArrayMetacentrum, IsMetacentrum\n",
    "\n",
    "#####################\n",
    "# pytorch & settings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda\")\n",
    "else:\n",
    "    print(\"Using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3212621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordedRun:\n",
    "    def __init__(self, npz_path, info):\n",
    "        super().__init__()\n",
    "\n",
    "        npz = np.load(npz_path)\n",
    "\n",
    "        self.dim = npz['dimensions']\n",
    "        self.fun = npz['function_id']\n",
    "\n",
    "        self.xmeans = npz['surrogate_data_means'].T\n",
    "        self.sigmas = npz['surrogate_data_sigmas']\n",
    "        self.bds = npz['surrogate_data_bds']\n",
    "        self.iruns = npz['iruns']\n",
    "        self.evals = npz['evals']\n",
    "        self.points = npz['points']\n",
    "        self.fvalues = npz['fvalues']\n",
    "        self.orig = npz['orig_evaled']\n",
    "        self.coco = npz['fvalues_orig']\n",
    "        self.gen_split = npz['gen_split']\n",
    "\n",
    "        self.n_gen = len(self.gen_split)\n",
    "        self.n_points = len(self.points)\n",
    "\n",
    "        # tuple: x_train, y_train, x_test, y_test, tss_mask, stats\n",
    "        self.all_gens = [self.get_gen(gen_i, info) for gen_i in range(1, self.n_gen)]\n",
    "        self.stats_table = pd.DataFrame([gen[-1] for gen in self.all_gens])\n",
    "\n",
    "        # for key, val in info.items():\n",
    "        #     self.stats_table[key] = val\n",
    "        # self.stats_table['ins'] = ins\n",
    "\n",
    "    def get_gen(self, gen_i, info):\n",
    "        # first point is initial guess\n",
    "        # gen_split[0] = 0, gen_split[1] = 1, gen_split[2] = 1, ...\n",
    "        low = self.gen_split[gen_i] + 1\n",
    "        high = self.gen_split[gen_i + 1] + 1 if gen_i + 1 < self.n_gen else self.n_points\n",
    "\n",
    "        x_test = self.points[low:high]\n",
    "        y_test = self.coco[low:high]\n",
    "\n",
    "        o = self.orig[:low]\n",
    "        x_train = self.points[:low][o]\n",
    "        y_train = self.coco[:low][o]\n",
    "\n",
    "        pop = x_test\n",
    "\n",
    "        mean = self.xmeans[gen_i]\n",
    "        sigma = self.sigmas[gen_i]\n",
    "        bd = self.bds[gen_i]\n",
    "        mahalanobis_transf = np.linalg.inv(bd * sigma)\n",
    "\n",
    "        maximum_distance = 4  # trainRange\n",
    "        maximum_number = int(20 * self.dim)\n",
    "\n",
    "        tss2 = tss.TSS2(pop, mahalanobis_transf, maximum_distance, maximum_number)\n",
    "        tss_mask, _ = tss2(x_train, y_train)\n",
    "\n",
    "        x_tss = x_train[tss_mask]\n",
    "        y_tss = y_train[tss_mask]\n",
    "\n",
    "        stats = {\n",
    "            \"gen_num\": gen_i,\n",
    "            \"restarts\": self.iruns[gen_i] - 1,\n",
    "            \"arch_len\": len(x_train),\n",
    "            \"tss_len\": np.count_nonzero(tss_mask),\n",
    "            \"var_y_tss\": np.var(y_tss),\n",
    "\n",
    "            \"sigma\": sigma,\n",
    "            \"bd\": bd,\n",
    "            \"mean\": mean,\n",
    "            #\"cond_num\": condition_num(bd.T),\n",
    "            #\"fst_scnd_ratio\": fst_scnd_ratio(bd.T),\n",
    "            #\"max_eigenvec\": max(np.linalg.norm(bd.T, axis=1))\n",
    "        }\n",
    "        stats.update(info)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test, tss_mask, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cfddc",
   "metadata": {},
   "source": [
    "### stuff used for ens and distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee131763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probabilistic_models import extend_info, \\\n",
    "    get_information_basic_model, get_information_ens, \\\n",
    "    get_information_distillation, get_information_double_distillation, \\\n",
    "    train_NLL_ensemble, train_ensemble, train_distillation, train_double_distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bb6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models_on_specific_state(state):\n",
    "    training_output = {}\n",
    "    \n",
    "    # dataset handling - train vs test - loading\n",
    "    x_train, y_train, x_test, y_test, tss_mask, stats = state\n",
    "            \n",
    "    x      = x_train[tss_mask].astype(np.float32)\n",
    "    y      = y_train[tss_mask].astype(np.float32).reshape(-1, 1)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # train-validation split\n",
    "    x_train, x_validation, y_train, y_validation = \\\n",
    "        sklearn.model_selection.train_test_split(x, y)\n",
    "        \n",
    "    # normalization x_train, y_train, x_validation, y_validation\n",
    "    x_normalizer = sklearn.preprocessing.StandardScaler().fit(x_train)\n",
    "    y_normalizer = sklearn.preprocessing.StandardScaler().fit(y_train)\n",
    "    \n",
    "    x_train      = x_normalizer.transform(x_train)\n",
    "    x_validation = x_normalizer.transform(x_validation)\n",
    "    x_test       = x_normalizer.transform(x_test)\n",
    "    \n",
    "    y_train      = y_normalizer.transform(y_train)\n",
    "    y_validation = y_normalizer.transform(y_validation)\n",
    "    y_test       = y_normalizer.transform(y_test)\n",
    "        \n",
    "    # make datasets and dataloader\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(x_train), torch.tensor(y_train))\n",
    "    validation_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(x_validation), torch.tensor(y_validation))\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=5, shuffle=True\n",
    "        )\n",
    "    validation_dataloader = torch.utils.data.DataLoader(\n",
    "            validation_dataset, batch_size=5, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # logging\n",
    "    training_output['size_train'] = len(x_train)\n",
    "    training_output['size_validation'] = len(x_validation)\n",
    "    training_output['size_test'] = len(x_test)\n",
    "    \n",
    "    # train ensemble\n",
    "    ensemble = train_NLL_ensemble(train_dataloader, validation_dataloader)\n",
    "\n",
    "    # ---------------------\n",
    "    \n",
    "    # 1. use only one model\n",
    "    model = ensemble[0]\n",
    "    training_output.update(\n",
    "        extend_info('basic', get_information_basic_model, model, x_test, y_test)\n",
    "    )\n",
    "    \n",
    "    # 2. use ensemble\n",
    "    model = train_ensemble(ensemble)\n",
    "    training_output.update(\n",
    "        extend_info('ensemble', get_information_ens, model, x_test, y_test)\n",
    "    )\n",
    "\n",
    "    # 3. use distillation\n",
    "    model = train_distillation(ensemble, train_dataloader, validation_dataloader)\n",
    "    training_output.update(\n",
    "        extend_info('distillation', get_information_distillation, model, x_test, y_test)\n",
    "    )\n",
    "\n",
    "    # 4. use double distillation\n",
    "    model = train_double_distillation(ensemble, train_dataloader, validation_dataloader)\n",
    "    training_output.update(\n",
    "        extend_info('doubledistillation', get_information_double_distillation, model, x_test, y_test)\n",
    "    )\n",
    "    \n",
    "    return training_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03367056",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = [\"name\", \"fun\", \"dim\", \"ker\", \"ins\", \"generation\"]\n",
    "for model in ['basic', 'ensemble', 'distillation', 'doubledistillation']:\n",
    "    for loss in [\"MSE\", \"MAE\", \"RDE\"]:\n",
    "        elements.append(model + \"_\" + loss)\n",
    "\n",
    "OUTPUT_TYPE = collections.namedtuple('ExperimentOutput', elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa16dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputfile(array, name):\n",
    "    path_input = run_folder + name\n",
    "    basename = os.path.splitext(name)[0]\n",
    "    path_output = output_folder + basename + '.csv'\n",
    "    path_output_npz = output_folder + basename + '.npz'\n",
    "    \n",
    "    fun, dim, rid, ins = name.split('.')[0].split('_')[-4:]\n",
    "    \n",
    "    fun = int(fun)\n",
    "    dim = int(dim[:-1])\n",
    "    ker = (int(rid) - 1) % 9\n",
    "    ins = int(ins)\n",
    "    \n",
    "    if os.path.exists(path_output):\n",
    "        print('Skipping ...' + path_output)\n",
    "        return\n",
    "    else:\n",
    "        print('Working ... ' + path_output)\n",
    "    \n",
    "    training_output = {\n",
    "                'name': name,\n",
    "                'fun': fun,\n",
    "                'dim': dim,\n",
    "                'ker': ker,\n",
    "                'ins': ins\n",
    "            }\n",
    "\n",
    "    record = RecordedRun(path_input, training_output)\n",
    "    \n",
    "    all_gens = list(filter(\n",
    "        lambda generation: len(generation[0]) >= 4, \n",
    "        record.all_gens\n",
    "    ))\n",
    "    \n",
    "    # parallel run\n",
    "    result = array.run_map(train_all_models_on_specific_state, all_gens[:4])\n",
    "    \n",
    "    with open(path_output, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        for ig, output in enumerate(result):\n",
    "            print(list(output.keys()))\n",
    "            output = OUTPUT_TYPE(**training_output, generation=ig, \n",
    "                    **{model + '_' + loss: output[model + '_' + loss] \n",
    "                        for model in ['basic', 'ensemble', 'distillation', 'doubledistillation'] \n",
    "                            for loss in [\"MSE\", \"MAE\", \"RDE\"]}\n",
    "                           )\n",
    "            writer.writerow(tuple(output))\n",
    "    print('savez')\n",
    "    np.savez_compressed(path_output_npz, data=result, allow_picke=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30abe149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working ... test_output/exp_doubleEC_28_log_nonadapt_results_1_2D_4_0.csv\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "savez\n",
      "Working ... test_output/exp_doubleEC_28_log_nonadapt_results_12_2D_103_4.csv\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "savez\n",
      "Working ... test_output/exp_doubleEC_28_log_nonadapt_results_15_5D_560_1.csv\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "savez\n",
      "Working ... test_output/exp_doubleEC_28_log_nonadapt_results_12_5D_540_1.csv\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "savez\n",
      "Working ... test_output/exp_doubleEC_28_log_nonadapt_results_15_3D_349_2.csv\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "['size_train', 'size_validation', 'size_test', 'basic_mean', 'basic_MSE', 'basic_MAE', 'basic_RDE', 'ensemble_mean', 'ensemble_MSE', 'ensemble_MAE', 'ensemble_RDE', 'distillation_mean', 'distillation_MSE', 'distillation_MAE', 'distillation_RDE', 'doubledistillation_mean', 'doubledistillation_MSE', 'doubledistillation_MAE', 'doubledistillation_RDE']\n",
      "savez\n",
      "Working ... test_output/exp_doubleEC_28_log_nonadapt_results_12_20D_969_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tumpji/bin/python_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3723: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/tumpji/bin/python_env/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/tumpji/bin/python_env/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#out = process_inputfile(array, run_files[0])\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39msplit_work(run_files):\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mprocess_inputfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mprocess_inputfile\u001b[0;34m(array, name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorking ... \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path_output)\n\u001b[1;32m     20\u001b[0m training_output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: name,\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: fun,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mins\u001b[39m\u001b[38;5;124m'\u001b[39m: ins\n\u001b[1;32m     26\u001b[0m         }\n\u001b[0;32m---> 28\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[43mRecordedRun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m all_gens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m generation: \u001b[38;5;28mlen\u001b[39m(generation[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, \n\u001b[1;32m     32\u001b[0m     record\u001b[38;5;241m.\u001b[39mall_gens\n\u001b[1;32m     33\u001b[0m ))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# parallel run\u001b[39;00m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mRecordedRun.__init__\u001b[0;34m(self, npz_path, info)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# tuple: x_train, y_train, x_test, y_test, tss_mask, stats\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_gens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gen(gen_i, info) \u001b[38;5;28;01mfor\u001b[39;00m gen_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_gen)]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([gen[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_gens])\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# tuple: x_train, y_train, x_test, y_test, tss_mask, stats\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_gens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m gen_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_gen)]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([gen[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_gens])\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mRecordedRun.get_gen\u001b[0;34m(self, gen_i, info)\u001b[0m\n\u001b[1;32m     53\u001b[0m maximum_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m     55\u001b[0m tss2 \u001b[38;5;241m=\u001b[39m tss\u001b[38;5;241m.\u001b[39mTSS2(pop, mahalanobis_transf, maximum_distance, maximum_number)\n\u001b[0;32m---> 56\u001b[0m tss_mask, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtss2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m x_tss \u001b[38;5;241m=\u001b[39m x_train[tss_mask]\n\u001b[1;32m     59\u001b[0m y_tss \u001b[38;5;241m=\u001b[39m y_train[tss_mask]\n",
      "File \u001b[0;32m~/Documents/2021/2022_ecml/Prior-CMAES/metacentrum/convert_full_data/tss.py:96\u001b[0m, in \u001b[0;36mTSS2.__call__\u001b[0;34m(self, archive_points, archive_evaluation, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, archive_points, archive_evaluation, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     distances, other_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marchive_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_evaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     D \u001b[38;5;241m=\u001b[39m archive_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     99\u001b[0m     N \u001b[38;5;241m=\u001b[39m archive_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/2021/2022_ecml/Prior-CMAES/metacentrum/convert_full_data/tss.py:32\u001b[0m, in \u001b[0;36mTSSbase.__call__\u001b[0;34m(self, archive_points, archive_evaluation, compute_distances, add_minimal_distances)\u001b[0m\n\u001b[1;32m     30\u001b[0m     differences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation[np\u001b[38;5;241m.\u001b[39mnewaxis, :, :] \u001b[38;5;241m-\u001b[39m archive_points[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# (O, G)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmahalanobis_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdifferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmahalanobis_transf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_not_square\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/2021/2022_ecml/Prior-CMAES/metacentrum/convert_full_data/tss.py:59\u001b[0m, in \u001b[0;36mTSSbase.mahalanobis_distance\u001b[0;34m(differences, mahalanobis_transf, do_not_square)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mahalanobis_transf\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (D, D)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# centered_points = np.matmul(differences, mahalanobis_transf)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m centered_points \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmahalanobis_transf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifferences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# # centered_points = np.matmul(differences, mahalanobis_transf)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# # TODO check\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# centered_points = np.matmul(mahalanobis_transf, differences[..., np.newaxis])\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# centered_points = np.matmul(differences[..., np.newaxis, :], centered_points)[..., 0]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# # pdb.set_trace()\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_not_square:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "#run_folder = '/storage/brno2/home/kozajan/data/bbob-cmaes-runs/'\n",
    "run_folder = 'data_full/'\n",
    "\n",
    "\n",
    "run_files = os.listdir(run_folder)\n",
    "run_files = natsorted(run_files)\n",
    "run_files = list(sorted(run_files, key=lambda name: int(name.split('.')[0].split('_')[-4:][1][:-1])))\n",
    "\n",
    "#output_folder = '/storage/brno2/home/tumpji/2022_esann/results/'\n",
    "output_folder = 'test_output/'\n",
    "\n",
    "\n",
    "array = ArrayMetacentrum(slice_type='random')\n",
    "\n",
    "#out = process_inputfile(array, run_files[0])\n",
    "\n",
    "for name in array.split_work(run_files):\n",
    "    process_inputfile(array, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d07e29",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'y' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(operator\u001b[38;5;241m.\u001b[39madd, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m t(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'y' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [
    "import operator, functools\n",
    "t = functools.partial(operator.add, b='+4')\n",
    "print(y=t)\n",
    "t('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74e986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
