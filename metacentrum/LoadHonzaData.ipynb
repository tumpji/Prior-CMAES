{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d212fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# io\n",
    "import os, sys, time\n",
    "import itertools, functools, collections\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "import csv\n",
    "\n",
    "sys.path.append('convert_full_data')\n",
    "sys.path.append('../regression-prior-networks')\n",
    "\n",
    "# science utils\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#####################\n",
    "# custom\n",
    "import tss\n",
    "from parallization_metacentrum import ArrayMetacentrum, IsMetacentrum\n",
    "\n",
    "#####################\n",
    "# pytorch & settings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda\")\n",
    "else:\n",
    "    print(\"Using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3212621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordedRun:\n",
    "    def __init__(self, npz_path, info):\n",
    "        super().__init__()\n",
    "\n",
    "        npz = np.load(npz_path)\n",
    "\n",
    "        self.dim = npz['dimensions']\n",
    "        self.fun = npz['function_id']\n",
    "\n",
    "        self.xmeans = npz['surrogate_data_means'].T\n",
    "        self.sigmas = npz['surrogate_data_sigmas']\n",
    "        self.bds = npz['surrogate_data_bds']\n",
    "        self.iruns = npz['iruns']\n",
    "        self.evals = npz['evals']\n",
    "        self.points = npz['points']\n",
    "        self.fvalues = npz['fvalues']\n",
    "        self.orig = npz['orig_evaled']\n",
    "        self.coco = npz['fvalues_orig']\n",
    "        self.gen_split = npz['gen_split']\n",
    "\n",
    "        self.n_gen = len(self.gen_split)\n",
    "        self.n_points = len(self.points)\n",
    "\n",
    "        # tuple: x_train, y_train, x_test, y_test, tss_mask, stats\n",
    "        self.all_gens = [self.get_gen(gen_i, info) for gen_i in range(1, self.n_gen)]\n",
    "        self.stats_table = pd.DataFrame([gen[-1] for gen in self.all_gens])\n",
    "\n",
    "        # for key, val in info.items():\n",
    "        #     self.stats_table[key] = val\n",
    "        # self.stats_table['ins'] = ins\n",
    "\n",
    "    def get_gen(self, gen_i, info):\n",
    "        # first point is initial guess\n",
    "        # gen_split[0] = 0, gen_split[1] = 1, gen_split[2] = 1, ...\n",
    "        low = self.gen_split[gen_i] + 1\n",
    "        high = self.gen_split[gen_i + 1] + 1 if gen_i + 1 < self.n_gen else self.n_points\n",
    "\n",
    "        x_test = self.points[low:high]\n",
    "        y_test = self.coco[low:high]\n",
    "\n",
    "        o = self.orig[:low]\n",
    "        x_train = self.points[:low][o]\n",
    "        y_train = self.coco[:low][o]\n",
    "\n",
    "        pop = x_test\n",
    "\n",
    "        mean = self.xmeans[gen_i]\n",
    "        sigma = self.sigmas[gen_i]\n",
    "        bd = self.bds[gen_i]\n",
    "        mahalanobis_transf = np.linalg.inv(bd * sigma)\n",
    "\n",
    "        maximum_distance = 4  # trainRange\n",
    "        maximum_number = int(20 * self.dim)\n",
    "\n",
    "        tss2 = tss.TSS2(pop, mahalanobis_transf, maximum_distance, maximum_number)\n",
    "        tss_mask, _ = tss2(x_train, y_train)\n",
    "\n",
    "        x_tss = x_train[tss_mask]\n",
    "        y_tss = y_train[tss_mask]\n",
    "\n",
    "        stats = {\n",
    "            \"gen_num\": gen_i,\n",
    "            \"restarts\": self.iruns[gen_i] - 1,\n",
    "            \"arch_len\": len(x_train),\n",
    "            \"tss_len\": np.count_nonzero(tss_mask),\n",
    "            \"var_y_tss\": np.var(y_tss),\n",
    "\n",
    "            \"sigma\": sigma,\n",
    "            \"bd\": bd,\n",
    "            \"mean\": mean,\n",
    "            #\"cond_num\": condition_num(bd.T),\n",
    "            #\"fst_scnd_ratio\": fst_scnd_ratio(bd.T),\n",
    "            #\"max_eigenvec\": max(np.linalg.norm(bd.T, axis=1))\n",
    "        }\n",
    "        stats.update(info)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test, tss_mask, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff314d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5280\n"
     ]
    }
   ],
   "source": [
    "run_folder = 'data_full/'\n",
    "run_files = os.listdir(run_folder)\n",
    "run_files = natsorted(run_files)\n",
    "print(len(run_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cfddc",
   "metadata": {},
   "source": [
    "### stuff used for ens and distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee131763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probabilistic_models import extend_info, \\\n",
    "    get_information_basic_model, get_information_ens, \\\n",
    "    get_information_distillation, get_information_double_distillation, \\\n",
    "    train_NLL_ensemble, train_ensemble, train_distillation, train_double_distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bb6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models_on_specific_state(state):\n",
    "    training_output = {}\n",
    "    \n",
    "    # dataset handling - train vs test - loading\n",
    "    x_train, y_train, x_test, y_test, tss_mask, stats = state\n",
    "            \n",
    "    x      = x_train[tss_mask].astype(np.float32)\n",
    "    y      = y_train[tss_mask].astype(np.float32).reshape(-1, 1)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # train-validation split\n",
    "    x_train, x_validation, y_train, y_validation = \\\n",
    "        sklearn.model_selection.train_test_split(x, y)\n",
    "        \n",
    "    # normalization x_train, y_train, x_validation, y_validation\n",
    "    x_normalizer = sklearn.preprocessing.StandardScaler().fit(x_train)\n",
    "    y_normalizer = sklearn.preprocessing.StandardScaler().fit(y_train)\n",
    "    \n",
    "    x_train      = x_normalizer.transform(x_train)\n",
    "    x_validation = x_normalizer.transform(x_validation)\n",
    "    x_test       = x_normalizer.transform(x_test)\n",
    "    \n",
    "    y_train      = y_normalizer.transform(y_train)\n",
    "    y_validation = y_normalizer.transform(y_validation)\n",
    "    y_test       = y_normalizer.transform(y_test)\n",
    "        \n",
    "    # make datasets and dataloader\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(x_train), torch.tensor(y_train))\n",
    "    validation_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(x_validation), torch.tensor(y_validation))\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=5, shuffle=True\n",
    "        )\n",
    "    validation_dataloader = torch.utils.data.DataLoader(\n",
    "            validation_dataset, batch_size=5, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # logging\n",
    "    training_output['size_train'] = len(x_train)\n",
    "    training_output['size_validation'] = len(x_validation)\n",
    "    training_output['size_test'] = len(x_test)\n",
    "    \n",
    "    # train ensemble\n",
    "    ensemble = train_NLL_ensemble(train_dataloader, validation_dataloader)\n",
    "\n",
    "    # ---------------------\n",
    "    \n",
    "    # 1. use only one model\n",
    "    model = ensemble[0]\n",
    "    training_output.update(\n",
    "        extend_info('basic', get_information_basic_model, model, x_test, y_test)\n",
    "    )\n",
    "    \n",
    "    # 2. use ensemble\n",
    "    model = train_ensemble(ensemble)\n",
    "    training_output.update(\n",
    "        extend_info('ensemble', get_information_ens, model, x_test, y_test)\n",
    "    )\n",
    "\n",
    "    # 3. use distillation\n",
    "    model = train_distillation(ensemble, train_dataloader, validation_dataloader)\n",
    "    training_output.update(\n",
    "        extend_info('distillation', get_information_distillation, model, x_test, y_test)\n",
    "    )\n",
    "\n",
    "    # 4. use double distillation\n",
    "    model = train_double_distillation(ensemble, train_dataloader, validation_dataloader)\n",
    "    training_output.update(\n",
    "        extend_info('doubledistillation', get_information_double_distillation, model, x_test, y_test)\n",
    "    )\n",
    "    \n",
    "    return training_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473f0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = [\"name\", \"fun\", \"dim\", \"ker\", \"ins\", \"generation\"]\n",
    "for model in ['basic', 'ensemble', 'distillation', 'doubledistillation']:\n",
    "    for loss in [\"MSE\", \"MAE\", \"RDE\"]:\n",
    "        elements.append(model + \"_\" + loss)\n",
    "\n",
    "OUTPUT_TYPE = collections.namedtuple('ExperimentOutput', elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9234071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputfile(array, name):\n",
    "    path_input = run_folder + name\n",
    "    basename = os.path.splitext(name)[0]\n",
    "    path_output = output_folder + basename + '.csv'\n",
    "    \n",
    "    fun, dim, rid, ins = name.split('.')[0].split('_')[-4:]\n",
    "    \n",
    "    fun = int(fun)\n",
    "    dim = int(dim[:-1])\n",
    "    ker = (int(rid) - 1) % 9\n",
    "    ins = int(ins)\n",
    "    \n",
    "    training_output = {\n",
    "                'name': name,\n",
    "                'fun': fun,\n",
    "                'dim': dim,\n",
    "                'ker': ker,\n",
    "                'ins': ins\n",
    "            }\n",
    "\n",
    "    record = RecordedRun(path_input, training_output)\n",
    "    \n",
    "    all_gens = list(filter(\n",
    "        lambda generation: len(generation[0]) >= 4, \n",
    "        record.all_gens\n",
    "    ))\n",
    "    \n",
    "    # parallel run\n",
    "    result = array.run_map(train_all_models_on_specific_state, all_gens)\n",
    "    \n",
    "    with open(path_output, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for ig, output in enumerate(result):\n",
    "            output = OUTPUT_TYPE(**training_output, generation=ig, \n",
    "                    **{model + '_' + loss: output[model + '_' + loss] \n",
    "                        for model in ['basic', 'ensemble', 'distillation', 'doubledistillation'] \n",
    "                            for loss in [\"MSE\", \"MAE\", \"RDE\"]}\n",
    "                           )\n",
    "            writer.writerow(tuple(output))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80963c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "run_folder = '/storage/brno2/home/kozajan/data/bbob-cmaes-runs/'\n",
    "run_files = os.listdir(run_folder)\n",
    "run_files = natsorted(run_files)\n",
    "run_files = list(sorted(run_files, key=lambda name: int(name.split('.')[0].split('_')[-4:][1][:-1])))\n",
    "\n",
    "output_folder = 'test_output/'\n",
    "\n",
    "array = ArrayMetacentrum(slice_type='offset')\n",
    "\n",
    "#out = process_inputfile(array, run_files[0])\n",
    "\n",
    "for name in array.split_work(run_files):\n",
    "    process_dataset(array, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
